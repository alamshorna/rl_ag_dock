{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b03f6681-4477-4afe-a406-bd29f977d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start, let's try to implement a REINFORCE algorithm (policy gradient)\n",
    "# we only need some sort of featurizer, \n",
    "# a policy graph network, \n",
    "# and the reinforcement learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed855911-7c18-4526-a48a-a9878d6e5097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # note: code borrowed from gaeun\n",
    "import os \n",
    "import yaml\n",
    "\n",
    "yamls_path = \"/project/liulab/gkim/antigen_prediction/eval_boltz_on_sabdab/all_yaml_outdir\"\n",
    "pdbs_path = \"/project/liulab/gkim/antigen_prediction/data/renumbered_sabdab_pdb_files/pdb_files\"\n",
    "\n",
    "def get_chain_info_from_pdb(pdb_path, yaml_path):\n",
    "    \"\"\"Get chain information from YAML file.\"\"\"    \n",
    "    if not os.path.exists(pdb_path):\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    if not os.path.exists(yaml_path):\n",
    "        print(f\"No YAML file found at {yaml_path}\")\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    try:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            yaml_data = yaml.safe_load(f)\n",
    "        \n",
    "        # Extract chain IDs and sequences from YAML data\n",
    "        # Assume the first sequence is heavy and the second sequence is light\n",
    "        # UNLESS there are more than 2 sequences\n",
    "        h_chain = None\n",
    "        l_chain = None\n",
    "        h_seq_yaml = None\n",
    "        l_seq_yaml = None\n",
    "\n",
    "        # Look for sequences in the YAML data\n",
    "        if 'sequences' in yaml_data and isinstance(yaml_data['sequences'], list):\n",
    "            sequences = yaml_data['sequences']\n",
    "            if len(sequences) == 2:\n",
    "                h_chain = sequences[0]['protein']['id']  # First sequence is heavy\n",
    "                l_chain = sequences[1]['protein']['id']  # Second sequence is light\n",
    "                h_seq_yaml = sequences[0]['protein']['sequence']\n",
    "                l_seq_yaml = sequences[1]['protein']['sequence']\n",
    "            elif len(sequences) > 2:\n",
    "                # first sequence is antigen (for multimer predictions)\n",
    "                h_chain = sequences[1]['protein']['id']  # Second sequence is heavy\n",
    "                l_chain = sequences[2]['protein']['id']  # Third sequence is light\n",
    "                h_seq_yaml = sequences[1]['protein']['sequence']\n",
    "                l_seq_yaml = sequences[2]['protein']['sequence']\n",
    "        \n",
    "        if 'antigen' in yaml_data and isinstance(yaml_data['antigen'], list):\n",
    "            antigen = yaml_data['antigen'][0]['protein']['sequence']\n",
    "        else:\n",
    "            antigen = None\n",
    "        \n",
    "        return h_chain, l_chain, h_seq_yaml, l_seq_yaml, antigen\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading YAML file for {yaml_path}: {e}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0625a31-544e-4346-9b53-b4f49da6c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sd_pd = pd.read_csv(\"sabdab_summary_all.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b37f5fd0-d4a2-4570-bacb-3c391a4a038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB.Polypeptide import three_to_index\n",
    "\n",
    "def featurizer(heavy_chain, ag_chain, residues, dist_matrix):\n",
    "    # https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95/\n",
    "    # node feature matrix with shape (number of nodes, number of features)\n",
    "    # graph connectivity (how the nodes are connected) with shape (2, number of directed edges)\n",
    "    # node ground-truth labels. In this problem, every node is assigned to one class (group)\n",
    "\n",
    "    # deduplicate while maintaining order\n",
    "    heavy_residues = list(dict.fromkeys(residues[0]))\n",
    "    ag_residues = list(dict.fromkeys(residues[1]))\n",
    "\n",
    "    node_features = torch.zeros(len(heavy_residues + ag_residues), 2)\n",
    "    heavy_idx_to_node_idx = {}\n",
    "    ag_idx_to_node_idx = {}\n",
    "    \n",
    "    # heavy chains are \"0\" and antigen chains are \"1\"\n",
    "    for i, res_idx in enumerate(heavy_residues + ag_residues):\n",
    "        if i < len(heavy_residues):\n",
    "            node_features[i][0] = 0\n",
    "            node_features[i][1] = three_to_index(heavy_chain[res_idx].get_resname())\n",
    "            heavy_idx_to_node_idx[res_idx] = i\n",
    "        else:\n",
    "            node_features[i][0] = 1\n",
    "            node_features[i][1] = three_to_index(ag_chain[res_idx].get_resname())\n",
    "            ag_idx_to_node_idx[res_idx] = i\n",
    "            \n",
    "    hc_nodes = torch.tensor([heavy_idx_to_node_idx[id] for id in residues[0]])\n",
    "    ag_nodes = torch.tensor([ag_idx_to_node_idx[id] for id in residues[1]])\n",
    "    edge_connections = torch.vstack((hc_nodes, ag_nodes)).T\n",
    "    \n",
    "    num_edges = edge_connections.shape[0]\n",
    "    edge_features = torch.zeros(num_edges, 1)\n",
    "    for j in range(num_edges):\n",
    "        a, b = edge_connections[j]\n",
    "        # this was created pre-deduplication\n",
    "        edge_features[j] = dist_matrix[residues[0][a], residues[1][b]].item()\n",
    "\n",
    "    return node_features, edge_connections, edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0fb75ee9-06bd-4840-95a8-926af773f03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing YAML files:   0%|          | 0/6920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217, 6)\n",
      "tensor([[ 6.2841],\n",
      "        [ 6.6791],\n",
      "        [12.4140],\n",
      "        [ 9.9115],\n",
      "        [ 8.2325],\n",
      "        [14.7043],\n",
      "        [10.7663],\n",
      "        [ 7.4657],\n",
      "        [ 6.9076],\n",
      "        [ 8.6688],\n",
      "        [ 9.2591],\n",
      "        [ 8.5089]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[215]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     45\u001b[39m bottom_k_indices = np.unravel_index(bottom_k, dist_matrix.shape)\n\u001b[32m     47\u001b[39m featurizer(heavy_chain, antigen_chain, bottom_k_indices, dist_matrix)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m\n",
      "\u001b[31mException\u001b[39m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "k = 12\n",
    "\n",
    "from Bio.PDB import PDBList, PDBParser, Select, PDBIO\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "\n",
    "parser = PDBParser(QUIET=True)\n",
    "\n",
    "for yaml_file in tqdm(os.listdir(yamls_path), desc=\"Processing YAML files\"):\n",
    "    yaml_path = os.path.join(yamls_path, yaml_file)\n",
    "    name = yaml_file.split('.')[0]\n",
    "    pdb_file = name + '.pdb'\n",
    "    pdb_path = os.path.join(pdbs_path, pdb_file)\n",
    "    # we'll use Gauen's function because it already maps from the name to the pdb that is already downloaded on the server...\n",
    "    h, l, _, _, _ = get_chain_info_from_pdb(pdb_path, yaml_path)\n",
    "    row = sd_pd[(sd_pd[\"pdb\"] == name) & (sd_pd[\"Hchain\"] == h) & (sd_pd[\"Lchain\"] == l)]\n",
    "    ag = row[\"antigen_chain\"].item() # this gives us the antigen chain alone!\n",
    "    structure = parser.get_structure(name, pdb_path)\n",
    "\n",
    "    # make a distance matrix\n",
    "    heavy_Cas = []\n",
    "    \n",
    "    # convert this into accessing entries in a generator?\n",
    "    heavy_chain = structure[0][h]\n",
    "    antigen_chain = structure[0][ag]\n",
    "\n",
    "    # construct the distance matrix\n",
    "    heavy_coords = np.array([res['CA'].coord for res in heavy_chain if 'CA' in res])\n",
    "    # heavy_residues = np.array([int(res.id[1]) for res in heavy_chain if 'CA' in res])\n",
    "    antigen_coords = np.array([res['CA'].coord for res in antigen_chain if 'CA' in res])\n",
    "    # antigen_residues = np.array([int(res.id[1]) for res in antigen_chain if 'CA' in res])\n",
    "    \n",
    "    dist_matrix = np.linalg.norm(\n",
    "        heavy_coords[:, np.newaxis, :] - antigen_coords[np.newaxis, :, :],\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    # https://numpy.org/devdocs/reference/generated/numpy.argpartition.html\n",
    "    # only sort the bottom k\n",
    "    bottom_k = np.argpartition(dist_matrix.flatten(), k)[:k]\n",
    "    # flatten and unravel :)\n",
    "    bottom_k_indices = np.unravel_index(bottom_k, dist_matrix.shape)\n",
    "\n",
    "    node_features, edge_connections, edge_features = featurizer(heavy_chain, antigen_chain, bottom_k_indices, dist_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8769710-5b8f-468b-a193-2248e5b87bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f948d-e095-4831-bcd1-31296af87cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl_ag_dock)",
   "language": "python",
   "name": "rl_ag_dock"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
