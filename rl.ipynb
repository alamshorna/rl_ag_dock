{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b03f6681-4477-4afe-a406-bd29f977d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start, let's try to implement a REINFORCE algorithm (policy gradient)\n",
    "# we only need some sort of featurizer, \n",
    "# a policy graph network, \n",
    "# and the reinforcement learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed855911-7c18-4526-a48a-a9878d6e5097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # note: code borrowed from gaeun\n",
    "import os \n",
    "import yaml\n",
    "\n",
    "yamls_path = \"/project/liulab/gkim/antigen_prediction/eval_boltz_on_sabdab/all_yaml_outdir\"\n",
    "pdbs_path = \"/project/liulab/gkim/antigen_prediction/data/renumbered_sabdab_pdb_files/pdb_files\"\n",
    "\n",
    "def get_chain_info_from_pdb(pdb_path, yaml_path):\n",
    "    \"\"\"Get chain information from YAML file.\"\"\"    \n",
    "    if not os.path.exists(pdb_path):\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    if not os.path.exists(yaml_path):\n",
    "        print(f\"No YAML file found at {yaml_path}\")\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    try:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            yaml_data = yaml.safe_load(f)\n",
    "        \n",
    "        # Extract chain IDs and sequences from YAML data\n",
    "        # Assume the first sequence is heavy and the second sequence is light\n",
    "        # UNLESS there are more than 2 sequences\n",
    "        h_chain = None\n",
    "        l_chain = None\n",
    "        h_seq_yaml = None\n",
    "        l_seq_yaml = None\n",
    "\n",
    "        # Look for sequences in the YAML data\n",
    "        if 'sequences' in yaml_data and isinstance(yaml_data['sequences'], list):\n",
    "            sequences = yaml_data['sequences']\n",
    "            if len(sequences) == 2:\n",
    "                h_chain = sequences[0]['protein']['id']  # First sequence is heavy\n",
    "                l_chain = sequences[1]['protein']['id']  # Second sequence is light\n",
    "                h_seq_yaml = sequences[0]['protein']['sequence']\n",
    "                l_seq_yaml = sequences[1]['protein']['sequence']\n",
    "            elif len(sequences) > 2:\n",
    "                # first sequence is antigen (for multimer predictions)\n",
    "                h_chain = sequences[1]['protein']['id']  # Second sequence is heavy\n",
    "                l_chain = sequences[2]['protein']['id']  # Third sequence is light\n",
    "                h_seq_yaml = sequences[1]['protein']['sequence']\n",
    "                l_seq_yaml = sequences[2]['protein']['sequence']\n",
    "        \n",
    "        if 'antigen' in yaml_data and isinstance(yaml_data['antigen'], list):\n",
    "            antigen = yaml_data['antigen'][0]['protein']['sequence']\n",
    "        else:\n",
    "            antigen = None\n",
    "        \n",
    "        return h_chain, l_chain, h_seq_yaml, l_seq_yaml, antigen\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading YAML file for {yaml_path}: {e}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0625a31-544e-4346-9b53-b4f49da6c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sd_pd = pd.read_csv(\"sabdab_summary_all.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f5fd0-d4a2-4570-bacb-3c391a4a038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB.Polypeptide import three_to_index\n",
    "\n",
    "def featurizer(heavy_chain, ag_chain, device='cpu'):\n",
    "    # https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95/\n",
    "    # node feature matrix with shape (number of nodes, number of features)\n",
    "    # graph connectivity (how the nodes are connected) with shape (2, number of directed edges)\n",
    "    # node ground-truth labels. In this problem, every node is assigned to one class (group)\n",
    "\n",
    "    # # construct the distance matrix\n",
    "    heavy_coords = np.array([res['CA'].coord for res in heavy_chain if 'CA' in res])\n",
    "    antigen_coords = np.array([res['CA'].coord for res in antigen_chain if 'CA' in res])\n",
    "    antigen_residues = np.array([int(res.id[1]) for res in antigen_chain if 'CA' in res])\n",
    "    \n",
    "    dist_matrix = np.linalg.norm(\n",
    "        heavy_coords[:, np.newaxis, :] - antigen_coords[np.newaxis, :, :],\n",
    "        axis=-1\n",
    "    )\n",
    "    # https://numpy.org/devdocs/reference/generated/numpy.argpartition.html\n",
    "    # only sort the bottom k\n",
    "    bottom_k = np.argpartition(dist_matrix.flatten(), k)[:k]\n",
    "    # flatten and unravel :)\n",
    "    bottom_k_indices = np.unravel_index(bottom_k, dist_matrix.shape)\n",
    "\n",
    "    residues = bottom_k_indices\n",
    "    \n",
    "    def matrix_idx_to_resnum(chain, matrix_idx_list):\n",
    "        ca_residues = [res for res in chain if 'CA' in res]\n",
    "        return [int(ca_residues[i].id[1]) for i in matrix_idx_list]\n",
    "\n",
    "    # Convert matrix indices → PDB residue numbers\n",
    "    heavy_residues = matrix_idx_to_resnum(heavy_chain, list(dict.fromkeys(residues[0])))\n",
    "    ag_residues    = matrix_idx_to_resnum(ag_chain, list(dict.fromkeys(residues[1])))\n",
    "    \n",
    "    # matrix indices (0..N-1) → PDB residue numbers\n",
    "    heavy_id_map = [int(res.id[1]) for res in heavy_chain if 'CA' in res]\n",
    "    ag_id_map    = [int(res.id[1]) for res in ag_chain if 'CA' in res]\n",
    "\n",
    "    # pdb residue numbers of the heavy and antigen residues\n",
    "    heavy_residues = [heavy_id_map[item.item()] for item in list(dict.fromkeys(residues[0]))]\n",
    "    ag_residues = [ag_id_map[item.item()] for item in list(dict.fromkeys(residues[1]))]\n",
    "\n",
    "    node_features = torch.zeros(len(heavy_residues + ag_residues), 2, device=device) # chain, residue_id\n",
    "    \n",
    "    heavy_idx_to_node_idx = {res_idx: i for i, res_idx in enumerate(heavy_residues)}\n",
    "    ag_idx_to_node_idx = {res_idx: i + len(heavy_residues) for i, res_idx in enumerate(ag_residues)}\n",
    "\n",
    "    # heavy chains are \"0\" and antigen chains are \"1\"\n",
    "    for i, res_idx in enumerate(heavy_residues + ag_residues):\n",
    "        if i < len(heavy_residues):\n",
    "            res_idx = heavy_id_map[i]\n",
    "            node_features[i][0] = 0\n",
    "            node_features[i][1] = three_to_index(heavy_chain[res_idx].get_resname())\n",
    "        else:\n",
    "            res_idx = ag_id_map[i - len(heavy_residues)]\n",
    "            node_features[i][0] = 1\n",
    "            node_features[i][1] = three_to_index(ag_chain[res_idx].get_resname())\n",
    "\n",
    "    # node_features = node_features.T\n",
    "    \n",
    "    hc_nodes = torch.tensor([heavy_idx_to_node_idx[matrix_idx_to_resnum(heavy_chain, [id.item()])[0]] \n",
    "                             for id in residues[0]], device=device)\n",
    "    ag_nodes = torch.tensor([ag_idx_to_node_idx[matrix_idx_to_resnum(ag_chain, [id.item()])[0]] \n",
    "                             for id in residues[1]], device=device)\n",
    "    # print(torch.vstack((hc_nodes, ag_nodes)).shape)\n",
    "    # print(torch.vstack((hc_nodes, ag_nodes)).T.shape)\n",
    "    # raise Exception\n",
    "    edge_connections = torch.vstack((hc_nodes, ag_nodes))\n",
    "    \n",
    "    num_edges = edge_connections.T.shape[0]\n",
    "    edge_features = torch.zeros(num_edges, 1, device=device)\n",
    "    for j, (a, b) in enumerate(zip(residues[0], residues[1])):\n",
    "        edge_features[j] = dist_matrix[a, b].item()\n",
    "    # edge_features = edge_features\n",
    "        \n",
    "    return node_features, edge_connections, edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fb75ee9-06bd-4840-95a8-926af773f03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing YAML files: 100%|██████████| 5/5 [00:00<00:00, 13.29it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "k = 12\n",
    "\n",
    "from Bio.PDB import PDBList, PDBParser, Select, PDBIO\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "\n",
    "parser = PDBParser(QUIET=True)\n",
    "\n",
    "for yaml_file in tqdm(os.listdir(yamls_path)[:5], desc=\"Processing YAML files\"):\n",
    "    yaml_path = os.path.join(yamls_path, yaml_file)\n",
    "    name = yaml_file.split('.')[0]\n",
    "    pdb_file = name + '.pdb'\n",
    "    pdb_path = os.path.join(pdbs_path, pdb_file)\n",
    "    # we'll use Gauen's function because it already maps from the name to the pdb that is already downloaded on the server...\n",
    "    h, l, _, _, _ = get_chain_info_from_pdb(pdb_path, yaml_path)\n",
    "    row = sd_pd[(sd_pd[\"pdb\"] == name) & (sd_pd[\"Hchain\"] == h) & (sd_pd[\"Lchain\"] == l)]\n",
    "    if row.empty:\n",
    "        continue\n",
    "    ag = row[\"antigen_chain\"].values[0] # this gives us the antigen chain alone!\n",
    "    structure = parser.get_structure(name, pdb_path)\n",
    "    if ag not in [chain.id for chain in structure[0]]:\n",
    "        continue\n",
    "\n",
    "    # # make a distance matrix\n",
    "    heavy_Cas = []\n",
    "    \n",
    "    # # convert this into accessing entries in a generator?\n",
    "    heavy_chain = structure[0][h]\n",
    "    antigen_chain = structure[0][ag]\n",
    "\n",
    "    node_features, edge_connections, edge_features = featurizer(heavy_chain, antigen_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8769710-5b8f-468b-a193-2248e5b87bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sa4139/rl_ag_dock/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# reference: https://medium.com/@volzhinnv/graph-convolutional-networks-gcn-all-you-need-to-know-code-implementation-fdfcde657b5c\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GINEConv\n",
    "from torch_geometric.nn import Sequential\n",
    "from torch_geometric.nn import global_add_pool\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINEConv.html?utm_source=chatgpt.com\n",
    "# class GCN(nn.Module):\n",
    "#     def __init__(self, num_node_features, hidden_dim, output_dimension):\n",
    "#         super(GCN, self).__init__()\n",
    "#         edge_mlp = nn.Sequential(\n",
    "#             nn.Linear(1, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim)\n",
    "#         )\n",
    "#         self.conv1 = GINEConv(nn.Linear(num_node_features, hidden_dim), edge_mlp)\n",
    "#         self.conv2 = GINEConv(nn.Linear(hidden_dim, hidden_dim), edge_mlp)\n",
    "#         self.conv3 = GINEConv(nn.Linear(hidden_dim, hidden_dim), edge_mlp)\n",
    "#         self.readout = nn.Linear(hidden_dim, output_dimension)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         x = self.conv1(x, edge_index, edge_attr)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x, edge_index, edge_attr)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv3(x, edge_index, edge_attr)\n",
    "#         x = global_add_pool(x, batch)\n",
    "#         x = self.readout(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee8b226b-5702-4c05-b4b7-75a53f5b7d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dim = 6 \n",
    "# model = GCN(num_node_features=node_features.size(1),\n",
    "#             hidden_dim=128,\n",
    "#             output_dimension=output_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19941208-f6ba-4c1d-9a54-762b1f6896fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.utils import seeding\n",
    "from torch_geometric.data import Data\n",
    "from numpy import pi\n",
    "from Bio.PDB.vectors import rotaxis2m\n",
    "from Bio.PDB.vectors import Vector\n",
    "\n",
    "class DockingEnv(gym.Env):\n",
    "    # metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, heavy_chain, antigen_chain, featurizer, k=12, device='cpu'):\n",
    "        self.heavy_chain = heavy_chain\n",
    "        self.antigen_chain = antigen_chain\n",
    "        self.featurizer = featurizer\n",
    "        self.device = device\n",
    "        self.action_space = Box(low = np.array([0, 0, 0, 0, 0, 0]), high = np.array([10, 10, 10, 2*np.pi, 2*np.pi, 2*np.pi], dtype=np.float32))\n",
    "        self.observation_space = None\n",
    "        self.starting_ag_coords = {atom.id: atom for res in self.antigen_chain for atom in res if 'CA' in res} # store the starting state of the antigen in terms of raw locations\n",
    "        \n",
    "    def step(self, action):\n",
    "        node_features, edge_index, edge_attr = self.featurizer(self.heavy_chain, self.antigen_chain, device=self.device)\n",
    "        state = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr) # store this as a Data type object!\n",
    "        # buffer.states.append(state)\n",
    "        trans = action[0:3]\n",
    "        rot = action[3:]\n",
    "        # print(action)\n",
    "        # multiply the rotation matrices wrt each of the directions\n",
    "        rotm = rotaxis2m(rot[0], Vector(1, 0, 0)) @ rotaxis2m(rot[1], Vector(0, 1, 0)) @ rotaxis2m(rot[2], Vector(0, 0, 1))\n",
    "        for atom in self.antigen_chain.get_atoms():\n",
    "            first_atom_coord = atom.coord\n",
    "            # going into a coordinate object, convert everything to numpy\n",
    "            atom.coord = rotm @ atom.coord + np.array(trans)\n",
    "            node_features, edge_index, edge_attr = self.featurizer(self.heavy_chain, self.antigen_chain, device=self.device)\n",
    "        next_state = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr) \n",
    "        return next_state, self._compute_reward(), 0, None\n",
    "        \n",
    "    def reset(self):\n",
    "        print(self._get_state())\n",
    "        for res in self.antigen_chain:\n",
    "            for atom in res:\n",
    "                # start with some big translation of the antigen chain\n",
    "                atom.coord = self.starting_ag_coords[atom.id].coord.copy() + np.array([-10, 0, 0])\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        node_features, edge_index, edge_attr = self.featurizer(self.heavy_chain, self.antigen_chain, device=self.device)\n",
    "        # print(edge_index.shape, edge_attr.shape)\n",
    "        return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    def _compute_reward(self):\n",
    "        node_features, edge_index, edge_attr = self.featurizer(self.heavy_chain, self.antigen_chain, device=self.device)\n",
    "        reward = -edge_attr.sum().item()\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b75c73ec-19b3-49a1-a328-414fc5274b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.dones = []\n",
    "\n",
    "    def store_transition(self, state, action, logprob, reward, done, state_value):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.logprobs.append(logprob)\n",
    "        self.rewards.append(reward)\n",
    "        self.state_values.append(state_value)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "        self.logprobs.clear()\n",
    "        self.rewards.clear()\n",
    "        self.state_values.clear()\n",
    "        self.dones.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2edcb46-335e-4f7f-a99a-9b61a7be93d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GINEConv, global_add_pool\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class GNNActorCritic(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim, action_dim, continuous_action=True):\n",
    "        super().__init__()\n",
    "        self.continuous_action = continuous_action\n",
    "        print(hidden_dim)\n",
    "\n",
    "        nn1 = nn.Sequential(nn.Linear(node_feature_dim, hidden_dim), nn.ReLU(),\n",
    "                            nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.conv1 = GINEConv(nn1, edge_dim=1)\n",
    "        nn2 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                            nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.conv2 = GINEConv(nn2, edge_dim=1)\n",
    "\n",
    "        self.feature_extractor = nn.ModuleList([self.conv1, self.conv2])\n",
    "\n",
    "        self.actor_head = nn.Linear(hidden_dim, action_dim)\n",
    "        self.critic_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = global_add_pool(x, batch)  # Graph-level embedding\n",
    "        return self.actor_head(x), self.critic_head(x)\n",
    "\n",
    "    def evaluate_actions(self, x, edge_index, edge_attr, batch, actions):\n",
    "        action_mean, state_values = self.forward(x, edge_index, edge_attr, batch)\n",
    "        dist = torch.distributions.Normal(action_mean, 0.1)  # std can be learnable\n",
    "        logprobs = dist.log_prob(actions).sum(-1)\n",
    "        entropy = dist.entropy().sum(-1)\n",
    "        return state_values.squeeze(), logprobs, entropy\n",
    "\n",
    "    def select_action(self, state):\n",
    "        batch = torch.zeros(state.x.size(0), dtype=torch.long, device=state.x.device)\n",
    "        action_mean, state = self.forward(state.x, state.edge_index, state.edge_attr, batch)\n",
    "        dist = Normal(action_mean, 0.1)\n",
    "        action = dist.sample().squeeze()\n",
    "        logprob = dist.log_prob(action).sum(-1)\n",
    "        return action, logprob\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93decc8c-2187-46ef-a91f-b437d20cdfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/saqib1707/RL-PPO-PyTorch\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "            self, \n",
    "            obs_dim, \n",
    "            action_dim, \n",
    "            hidden_dim, \n",
    "            lr_actor, \n",
    "            lr_critic, \n",
    "            continuous_action_space=False, \n",
    "            num_epochs=10, \n",
    "            eps_clip=0.2, \n",
    "            action_std_init=0.6, \n",
    "            gamma=0.99,\n",
    "            entropy_coef=0.01,\n",
    "            value_loss_coef=0.5,\n",
    "            batch_size=64,\n",
    "            max_grad_norm=0.5,\n",
    "            device='cpu'\n",
    "        ):\n",
    "        self.gamma = gamma\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_clip = eps_clip\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_std_init = action_std_init\n",
    "        self.continuous_action_space = continuous_action_space\n",
    "        self.device = device\n",
    "\n",
    "        self.policy = GNNActorCritic(\n",
    "            node_feature_dim=obs_dim,  # this is the node feature size\n",
    "            hidden_dim=hidden_dim,\n",
    "            action_dim=action_dim,\n",
    "            continuous_action=True  # we want continuous 6D actions\n",
    "        ).to(device)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.policy.feature_extractor.parameters()},\n",
    "            {'params': self.policy.actor_head.parameters(), 'lr': lr_actor},\n",
    "            {'params': self.policy.critic_head.parameters(), 'lr': lr_critic}\n",
    "        ])\n",
    "\n",
    "        self.buffer = RolloutBuffer()\n",
    "        self.mse_loss = nn.MSELoss()  # Initialize MSE loss\n",
    "\n",
    "\n",
    "    def compute_returns(self):\n",
    "        returns = []\n",
    "        discounted_reward = 0\n",
    "\n",
    "        for reward, done in zip(reversed(self.buffer.rewards), reversed(self.buffer.dones)):\n",
    "            if done:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + self.gamma * discounted_reward\n",
    "            returns.insert(0, discounted_reward)\n",
    "\n",
    "        returns = np.array(returns, dtype=np.float32)\n",
    "        returns = torch.flatten(torch.from_numpy(returns).float()).to(self.device)\n",
    "        return returns\n",
    "\n",
    "\n",
    "    def update_policy(self):\n",
    "        # print(len(self.buffer.rewards))\n",
    "        rewards_to_go = self.compute_returns()\n",
    "        # print(len(rewards_to_go))\n",
    "\n",
    "        # Handle Data objects - batch them instead of converting to numpy\n",
    "        from torch_geometric.data import Batch\n",
    "        states_list = [s.to(self.device) for s in self.buffer.states]\n",
    "        \n",
    "        # Convert actions, logprobs, state_vals to tensors on device\n",
    "        actions_list = []\n",
    "        for action in self.buffer.actions:\n",
    "            if isinstance(action, torch.Tensor):\n",
    "                actions_list.append(action.cpu().numpy() if action.is_cuda else action.numpy())\n",
    "            else:\n",
    "                actions_list.append(action)\n",
    "        actions = torch.from_numpy(np.array(actions_list, dtype=np.float32)).to(self.device)\n",
    "        \n",
    "        logprobs_list = []\n",
    "        for logprob in self.buffer.logprobs:\n",
    "            if isinstance(logprob, torch.Tensor):\n",
    "                logprobs_list.append(logprob.cpu().item() if logprob.is_cuda else logprob.item())\n",
    "            else:\n",
    "                logprobs_list.append(logprob)\n",
    "        old_logprobs = torch.from_numpy(np.array(logprobs_list, dtype=np.float32)).to(self.device)\n",
    "        \n",
    "        state_vals_list = []\n",
    "        for sv in self.buffer.state_values:\n",
    "            if isinstance(sv, torch.Tensor):\n",
    "                state_vals_list.append(sv.cpu().item() if sv.is_cuda else sv.item())\n",
    "            else:\n",
    "                state_vals_list.append(sv)\n",
    "        state_vals = torch.from_numpy(np.array(state_vals_list, dtype=np.float32)).to(self.device)\n",
    "\n",
    "        # print('stage-0:', rewards_to_go.shape, state_vals.shape)\n",
    "        # print('stage-1:', rewards_to_go.device, state_vals.device)\n",
    "        advantages = rewards_to_go - state_vals\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)\n",
    "\n",
    "        # print(states.shape, actions.shape, old_logprobs.shape, state_vals.shape, advantages.shape, rewards_to_go.shape)\n",
    "\n",
    "        for _ in range(self.num_epochs):\n",
    "            # generate random indices for minibatch\n",
    "            indices = np.random.permutation(len(self.buffer.states))\n",
    "\n",
    "            for start_idx in range(0, len(self.buffer.states), self.batch_size):\n",
    "                end_idx = start_idx + self.batch_size\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "\n",
    "                # Batch Data objects\n",
    "                batch_states = Batch.from_data_list([states_list[i] for i in batch_indices])\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_logprobs = old_logprobs[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_rewards_to_go = rewards_to_go[batch_indices]\n",
    "                \n",
    "                # evaluate old actions and values\n",
    "                state_values, logprobs, dist_entropy = self.policy.evaluate_actions(\n",
    "                    batch_states.x, batch_states.edge_index, batch_states.edge_attr, batch_states.batch, batch_actions\n",
    "                )\n",
    "                # print(logprobs.shape, batch_old_logprobs.shape)\n",
    "\n",
    "                # Finding the ratio (pi_theta / pi_theta_old)\n",
    "                ratios = torch.exp(logprobs - batch_old_logprobs.squeeze(-1))\n",
    "\n",
    "                # Finding Surrogate Loss\n",
    "                # print(ratios.shape, batch_advantages.shape)\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * batch_advantages\n",
    "\n",
    "                # final loss of clipped objective PPO\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                # print(state_values.dtype, batch_rewards_to_go.dtype)\n",
    "                critic_loss = 0.5 * self.mse_loss(state_values.squeeze(), batch_rewards_to_go)\n",
    "                loss = actor_loss + self.value_loss_coef * critic_loss - self.entropy_coef * dist_entropy.mean()\n",
    "                # print(\"Final loss:\", actor_loss, critic_loss, dist_entropy, loss)\n",
    "\n",
    "                # calculate gradients and backpropagate for actor network\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        \n",
    "        self.buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e798214-9ee8-435d-ba83-11750717aa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# code taken from https://github.com/saqib1707/RL-PPO-PyTorch/blob/main/src/model.py\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim import Adam\n",
    "# import gym\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, hidden_size=64):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(inp_dim, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        # convert observation to tensor if it's a numpy array\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "\n",
    "        x = self.relu(self.layer1(obs))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        out = self.layer3(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ProximalPolicyOptimization:\n",
    "    def __init__(self, env, seed=43, lr=1e-3):\n",
    "        assert type(env.observation_space) == Box, \"This example only works for envs with continuous state spaces.\"\n",
    "        assert type(env.action_space) == Box, \"This example only works for envs with continuous action spaces.\"\n",
    "        self._set_seed(seed)\n",
    "\n",
    "        # extract environment information\n",
    "        self.env = env\n",
    "        self.obs_dim = env.observation_space.shape[0]    # = ns\n",
    "        self.act_dim = env.action_space.shape[0]    # = na\n",
    "        print(f\"Observation Dimension: {self.obs_dim} | Action Dimension: {self.act_dim}\")\n",
    "\n",
    "        # initialize actor and critic networks\n",
    "        self.actor = FeedForwardNN(inp_dim=self.obs_dim, out_dim=self.act_dim)\n",
    "        self.critic = FeedForwardNN(inp_dim=self.obs_dim, out_dim=1)\n",
    "\n",
    "        self.actor_optimizer = Adam(self.actor.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "        self.critic_optimizer = Adam(self.critic.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "        # initialize action covariance matrix for exploration\n",
    "        self.act_cov = torch.diag(torch.full(size=(self.act_dim,), fill_value=0.5))    # (na,na)\n",
    "        # print(self.action_cov_mat)\n",
    "\n",
    "        # initialize logger\n",
    "        self.logger = {\n",
    "            'delta_t': time.time_ns(),\n",
    "            't_so_far': 0,\n",
    "            'i_so_far': 0,\n",
    "            'batch_lens': [],\n",
    "            'batch_rewards': [],\n",
    "            'actor_losses': [],\n",
    "        }\n",
    "\n",
    "\n",
    "    def learn(self, total_timesteps, timesteps_per_batch, max_eps_len, num_updates_per_itr, clip_thresh=0.2, save_every=1000, gamma=0.9):\n",
    "        t_so_far = 0    # timesteps simulated so far\n",
    "        i_so_far = 0\n",
    "\n",
    "        while t_so_far < total_timesteps:\n",
    "            # roll out multiple trajectories\n",
    "            batch_obs, batch_actions, batch_logprobs, batch_reward_to_go, batch_eps_lens = self.collect_rollouts(\n",
    "                timesteps_per_batch, \n",
    "                max_eps_len, \n",
    "                gamma\n",
    "            )\n",
    "            print(\"stage-1:\", batch_obs.shape, batch_actions.shape, batch_logprobs.shape, batch_reward_to_go.shape)\n",
    "\n",
    "            # calculate how many timesteps collected in this batch\n",
    "            t_so_far += np.sum(batch_eps_lens)\n",
    "            i_so_far += 1\n",
    "\n",
    "            # logging timesteps and iterations so far\n",
    "            self.logger['t_so_far'] = t_so_far\n",
    "            self.logger['i_so_far'] = i_so_far\n",
    "\n",
    "            # calculate value function V_{phi, k} using critic model\n",
    "            V, _ = self.evaluate(batch_obs, batch_actions)\n",
    "\n",
    "            # calculate advantage function A_k\n",
    "            A_k = batch_reward_to_go - V.detach()\n",
    "\n",
    "            # normalize advantage function\n",
    "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "\n",
    "            for _ in range(num_updates_per_itr):\n",
    "                # calculate pi_theta(at | st)\n",
    "                curr_V, curr_logprobs = self.evaluate(batch_obs, batch_actions)\n",
    "\n",
    "                # calcuate ratios\n",
    "                ratios = torch.exp(curr_logprobs - batch_logprobs)\n",
    "\n",
    "                # calcuate surrogate losses\n",
    "                surr1 = ratios * A_k\n",
    "\n",
    "                # clips ratio to make sure we are not stepping too far in any direction during gradient ascent\n",
    "                surr2 = torch.clamp(ratios, 1 - clip_thresh, 1 + clip_thresh) * A_k\n",
    "\n",
    "                # calculate actor and critic losses\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                critic_loss = nn.MSELoss()(curr_V, batch_reward_to_go)\n",
    "\n",
    "                # calculate gradients and backpropagate for actor network\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward(retain_graph=True)\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # calculate gradients and backpropagate for critic network\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "                self.logger['actor_losses'].append(actor_loss.detach())\n",
    "            \n",
    "            # print a summary of the training so far\n",
    "            self._log_summary(total_timesteps)\n",
    "\n",
    "            if i_so_far % save_every == 0:\n",
    "                torch.save(self.actor.state_dict(), './checkpoints/ppo_actor.pth')\n",
    "                torch.save(self.critic.state_dict(), './checkpoints/ppo_critic.pth')\n",
    "\n",
    "\n",
    "    def evaluate(self, batch_obs, batch_actions):\n",
    "        value = self.critic(batch_obs).squeeze()\n",
    "        # print(value.shape)\n",
    "\n",
    "        # calculate the log probabilities of batch actions using most recent actor network\n",
    "        mean = self.actor(batch_obs)\n",
    "        # print(\"Stage-2\", mean.shape, self.action_cov_mat.shape, batch_obs.shape, batch_actions.shape)\n",
    "        dist = MultivariateNormal(mean, self.act_cov)\n",
    "        # print(\"This would be printed\", dist)\n",
    "        logprob = dist.log_prob(batch_actions)\n",
    "        # print(\"This would not be printed\", dist)\n",
    "        return value, logprob\n",
    "\n",
    "\n",
    "    def collect_rollouts(self, max_timesteps, max_eps_len, gamma):\n",
    "        observations = []\n",
    "        actions = []\n",
    "        logprobs = []\n",
    "        rewards = []\n",
    "        eps_lens = []\n",
    "\n",
    "        t = 0\n",
    "        while t < max_timesteps:\n",
    "            # reset environment and get initial observation\n",
    "            obs, _ = self.env.reset()\n",
    "            done = False\n",
    "            # print(\"Stage-2 after reset:\", obs)\n",
    "\n",
    "            eps_rewards = []\n",
    "            for step in range(max_eps_len):\n",
    "                action, logprob = self.select_action(obs)\n",
    "                next_obs, reward, done, _, _ = self.env.step(action)\n",
    "                t += 1\n",
    "\n",
    "                # collect observation, action, log probabilities and reward\n",
    "                observations.append(obs)\n",
    "                actions.append(action)\n",
    "                logprobs.append(logprob)\n",
    "                eps_rewards.append(reward)\n",
    "\n",
    "                obs = next_obs\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # collect episode length and rewards\n",
    "            rewards.append(eps_rewards)\n",
    "            eps_lens.append(step+1)\n",
    "\n",
    "        # reshape numpy data as tensors\n",
    "        observations = torch.from_numpy(np.array(observations, dtype=np.float32))    # [max_timesteps, ns]\n",
    "        actions = torch.from_numpy(np.array(actions, dtype=np.float32))    # [max_timesteps, na]\n",
    "        actions = actions.unsqueeze(1)\n",
    "        logprobs = torch.from_numpy(np.array(logprobs, dtype=np.float32))    # [max_timesteps]\n",
    "        rewards_to_go = self.compute_reward_to_go(rewards, gamma)\n",
    "        # print(\"Stage-0:\", np.array(batch_rewards).shape, batch_reward_to_go.shape)\n",
    "        # batch_episode_lengths = torch.tensor(batch_episode_lengths, dtype=torch.float32)\n",
    "\n",
    "        # log the episodic rewards and lengths\n",
    "        self.logger['batch_rewards'] = rewards\n",
    "        self.logger['batch_lengths'] = eps_lens\n",
    "        return observations, actions, logprobs, rewards_to_go, eps_lens\n",
    "\n",
    "\n",
    "    def compute_reward_to_go(self, rewards, gamma):\n",
    "        \"\"\"\n",
    "        Compute the discounted reward-to-go for each timestep in each episode\n",
    "        Args:\n",
    "            rewards: list of lists, where each inner list contains rewards for an episode\n",
    "            gamma: discount  for future rewards\n",
    "        Returns:\n",
    "            rewards_to_go: list of reward-to-go for each timestep in each episode\n",
    "        \"\"\"\n",
    "        rewards_to_go = []\n",
    "\n",
    "        # iterate through each episodic rewards\n",
    "        for eps_rewards in rewards:\n",
    "            eps_rewards_to_go = []\n",
    "            reward_sum = 0\n",
    "\n",
    "            for r in reversed(eps_rewards):\n",
    "                reward_sum = r + gamma * reward_sum    # discounted reward\n",
    "                eps_rewards_to_go.append(reward_sum)\n",
    "\n",
    "            eps_rewards_to_go = eps_rewards_to_go[::-1]\n",
    "            rewards_to_go.append(eps_rewards_to_go)\n",
    "\n",
    "        # convert reward-to-go into tensor\n",
    "        rewards_to_go = np.array(rewards_to_go, dtype=np.float32)\n",
    "        rewards_to_go = torch.flatten(torch.from_numpy(rewards_to_go))\n",
    "\n",
    "        return rewards_to_go\n",
    "\n",
    "\n",
    "    def estimate_action(self, obs):\n",
    "        print(\"Stage-3:\", obs)\n",
    "        # query the actor network for mean of the distribution\n",
    "        mean = self.actor(obs)\n",
    "\n",
    "        # create multivariate normal distribution\n",
    "        dist = MultivariateNormal(mean, self.act_cov)\n",
    "\n",
    "        # sample an action from the distribution and compute its logprob\n",
    "        action = dist.sample()\n",
    "        logprob = dist.log_prob(action)\n",
    "\n",
    "        return action.detach().numpy(), logprob.detach()\n",
    "\n",
    "\n",
    "    def _set_seed(self, seed):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        print(f\"Successfully set seed everywhere: {seed}\")\n",
    "\n",
    "\n",
    "    def _log_summary(self, total_timesteps):\n",
    "        delta_t = self.logger['delta_t']\n",
    "        self.logger['delta_t'] = time.time_ns()\n",
    "        delta_t = round((self.logger['delta_t'] - delta_t) / 1e9, 4)\n",
    "\n",
    "        avg_episode_lens = np.mean(self.logger['batch_lengths'])\n",
    "        avg_episode_rewards = round(np.mean([np.sum(ep_rewards) for ep_rewards in self.logger['batch_rewards']]), 4)\n",
    "        avg_actor_loss = round(np.mean([losses.mean() for losses in self.logger['actor_losses']]), 4)\n",
    "\n",
    "        print(f\"{self.logger['t_so_far']}/{total_timesteps} | Avg Loss: {avg_actor_loss} | Avg Ep Len: {avg_episode_lens} | Avg Ep Reward: {avg_episode_rewards} | Itr {self.logger['i_so_far']} took {delta_t} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de673a8e-6a1a-4ef1-ab60-e3b5de12b8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[13, 2], edge_index=[2, 12], edge_attr=[12, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got mat1 is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m ep_reward = \u001b[32m0\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     action, logprob = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     next_state, reward, done, _ = env.step(action)\n\u001b[32m     24\u001b[39m     ep_reward += \u001b[32m0\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mGNNActorCritic.select_action\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[32m     38\u001b[39m     batch = torch.zeros(state.x.size(\u001b[32m0\u001b[39m), dtype=torch.long, device=state.x.device)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     action_mean, state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     dist = Normal(action_mean, \u001b[32m0.1\u001b[39m)\n\u001b[32m     41\u001b[39m     action = dist.sample().squeeze()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mGNNActorCritic.forward\u001b[39m\u001b[34m(self, x, edge_index, edge_attr, batch)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_attr, batch):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.conv2(x, edge_index, edge_attr)\n\u001b[32m     27\u001b[39m     x = global_add_pool(x, batch)  \u001b[38;5;66;03m# Graph-level embedding\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/sa4139/rl_ag_dock/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/sa4139/rl_ag_dock/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/sa4139/rl_ag_dock/.venv/lib/python3.12/site-packages/torch_geometric/nn/conv/gin_conv.py:187\u001b[39m, in \u001b[36mGINEConv.forward\u001b[39m\u001b[34m(self, x, edge_index, edge_attr, size)\u001b[39m\n\u001b[32m    184\u001b[39m     x = (x, x)\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m=\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m x_r = x[\u001b[32m1\u001b[39m]\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/torch_geometric.nn.conv.gin_conv_GINEConv_propagate_8c4ce7ed.py:183\u001b[39m, in \u001b[36mpropagate\u001b[39m\u001b[34m(self, edge_index, x, edge_attr, size)\u001b[39m\n\u001b[32m    174\u001b[39m             kwargs = CollectArgs(\n\u001b[32m    175\u001b[39m                 x_j=hook_kwargs[\u001b[33m'\u001b[39m\u001b[33mx_j\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    176\u001b[39m                 edge_attr=hook_kwargs[\u001b[33m'\u001b[39m\u001b[33medge_attr\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    179\u001b[39m                 dim_size=kwargs.dim_size,\n\u001b[32m    180\u001b[39m             )\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/sa4139/rl_ag_dock/.venv/lib/python3.12/site-packages/torch_geometric/nn/conv/gin_conv.py:202\u001b[39m, in \u001b[36mGINEConv.message\u001b[39m\u001b[34m(self, x_j, edge_attr)\u001b[39m\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNode and edge feature dimensionalities do not \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33mmatch. Consider setting the \u001b[39m\u001b[33m'\u001b[39m\u001b[33medge_dim\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33mattribute of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mGINEConv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     edge_attr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlin\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (x_j + edge_attr).relu()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/sa4139/rl_ag_dock/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/sa4139/rl_ag_dock/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/sa4139/rl_ag_dock/.venv/lib/python3.12/site-packages/torch_geometric/nn/dense/linear.py:127\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m    122\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[33;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but got mat1 is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "node_feature_dim = 2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "env = DockingEnv(heavy_chain, antigen_chain, featurizer, device=device)\n",
    "agent = PPOAgent(\n",
    "    obs_dim=node_feature_dim,\n",
    "    action_dim=6,\n",
    "    hidden_dim=128,\n",
    "    lr_actor=1e-3,\n",
    "    lr_critic=1e-3,\n",
    "    continuous_action_space=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "num_episodes = 3\n",
    "rewards = []\n",
    "for ep in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    while not done:\n",
    "        action, logprob = agent.policy.select_action(state)\n",
    "        \n",
    "        # Compute state value\n",
    "        batch = torch.zeros(state.x.size(0), dtype=torch.long, device=state.x.device)\n",
    "        _, state_value = agent.policy.forward(state.x, state.edge_index, state.edge_attr, batch)\n",
    "        state_value = state_value.squeeze().item()\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action.detach().cpu().numpy() if isinstance(action, torch.Tensor) else action)\n",
    "        ep_reward += 0\n",
    "        \n",
    "        # Save in buffer\n",
    "        agent.buffer.states.append(state)\n",
    "        agent.buffer.actions.append(action.detach().cpu().numpy() if isinstance(action, torch.Tensor) else action)\n",
    "        agent.buffer.rewards.append(reward)\n",
    "        agent.buffer.dones.append(done)\n",
    "        agent.buffer.logprobs.append(logprob.detach().cpu().item() if isinstance(logprob, torch.Tensor) else logprob)\n",
    "        agent.buffer.state_values.append(state_value)\n",
    "        \n",
    "        state = next_state\n",
    "    rewards.append(ep_reward)\n",
    "    agent.update_policy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl_ag_dock)",
   "language": "python",
   "name": "rl_ag_dock"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
